from typing import List
import json
from core.file_tools import _aura_safe_loads
from core.logging_utils import log_json

class CriticAgent:
    """
    The CriticAgent is responsible for evaluating plans, code, and mutation proposals
    generated by other agents. It provides constructive feedback and validates
    changes to ensure they align with the system's objectives and quality standards.
    """
    def __init__(self, brain, model):
        """
        Initializes the CriticAgent with access to the system's brain and model.

        Args:
            brain: An instance of the system's memory (Brain).
            model: An instance of the model adapter for LLM interactions.
        """
        self.brain = brain
        self.model = model

    def critique_plan(self, task: str, plan: List[str]) -> str:
        """
        Critiques a given plan against a high-level task.

        Args:
            task (str): The high-level task the plan is supposed to address.
            plan (List[str]): The detailed plan (list of steps) to be critiqued.

        Returns:
            str: The LLM's feedback and suggestions for improving the plan.
        """
        prompt = f"""
You are an autonomous critic agent. Your task is to evaluate a given plan against a high-level task.
Provide constructive feedback to improve the plan.

High-level Task:
{task}

Plan to critique:
{"\n".join(plan)}

Previous memory:
{chr(10).join(self.brain.recall_with_budget(max_tokens=1500))}

Evaluate the plan for completeness, clarity, feasibility, and alignment with the high-level task.
Suggest improvements or identify missing steps.
"""
        response = self.model.respond(prompt)
        self.brain.remember(f"Critiqued plan for task: {task}. Feedback: {response}")
        return response

    def critique_code(self, task: str, code: str, requirements: str = "") -> str:
        """
        Critiques provided code against a given task and requirements.

        Args:
            task (str): The high-level task the code is supposed to fulfill.
            code (str): The Python code to be critiqued.
            requirements (str, optional): Specific requirements or criteria for the code. Defaults to "".

        Returns:
            str: The LLM's feedback and suggestions for improving the code.
        """
        prompt = f"""
You are an autonomous critic agent. Your task is to evaluate provided code against a given task and requirements.
Provide constructive feedback to improve the code.

Task:
{task}

Code to critique:
```python
{code}
```

Requirements:
{requirements if requirements else "No specific requirements provided beyond the task."}

Previous memory:
{chr(10).join(self.brain.recall_with_budget(max_tokens=1500))}

Evaluate the code for correctness, efficiency, readability, maintainability, and adherence to requirements.
Suggest improvements or identify potential issues.
"""
        response = self.model.respond(prompt)
        self.brain.remember(f"Critiqued code for task: {task}. Feedback: {response}")
        return response

    def validate_mutation(self, mutation_proposal: str) -> str:
        """
        Validates a proposed system mutation, assessing its potential impact,
        safety, and effectiveness. The LLM is prompted to return a structured
        JSON object with decision, confidence, impact, and reasoning.

        Args:
            mutation_proposal (str): The details of the mutation proposed by another agent.

        Returns:
            str: A JSON string representing the LLM's structured validation response.
                 Includes "decision", "confidence_score", "impact_assessment", and "reasoning".
        """
        prompt = f"""
        You are a highly analytical critic agent responsible for validating proposed system mutations.
        Your goal is to assess the potential impact, safety, and effectiveness of a mutation.

        Mutation Proposal to Validate:
        ---
        {mutation_proposal}
        ---

        Based on your analysis, provide a structured JSON object with the following keys:
        
        {{
            "decision": "[APPROVED|REJECTED]",
            "confidence_score": "[0.0-1.0]",
            "impact_assessment": "(Briefly describe the anticipated positive and negative impacts, e.g., 'High confidence in fixing X, but might introduce risk Y.')",
            "reasoning": "(Detailed explanation for your decision, including any potential risks or benefits.)"
        }}
        Ensure your response contains ONLY the JSON object, with no conversational text or other explanations.
        """
        validation_response = self.model.respond(prompt)
        self.brain.remember(f"Validated mutation: {mutation_proposal}. Raw response: {validation_response[:100]}...")
        
        try:
            parsed_validation = _aura_safe_loads(validation_response, "critic_mutation_validation")
            # Basic validation of the parsed structure
            if isinstance(parsed_validation, dict) and all(key in parsed_validation for key in ["decision", "confidence_score", "impact_assessment", "reasoning"]):
                return json.dumps(parsed_validation) # Return as JSON string for consistency
            else:
                log_json("ERROR", "critic_validation_response_invalid_format", details={"response_snippet": validation_response[:200], "parsed_type": type(parsed_validation).__name__})
                return json.dumps({"decision": "REJECTED", "confidence_score": 0.0, "impact_assessment": "LLM returned invalid format", "reasoning": f"LLM returned invalid format: {validation_response}"})
        except json.JSONDecodeError as e:
            log_json("ERROR", "critic_validation_json_decode_error", details={"error": str(e), "response_snippet": validation_response[:200]})
            return json.dumps({"decision": "REJECTED", "confidence_score": 0.0, "impact_assessment": "JSON decode failed", "reasoning": f"Failed to decode JSON from critic response: {e}. Raw response: {validation_response}"})
        except Exception as e:
            log_json("ERROR", "critic_validation_unexpected_error", details={"error": str(e), "response_snippet": validation_response[:200]})
            return json.dumps({"decision": "REJECTED", "confidence_score": 0.0, "impact_assessment": "Unexpected error", "reasoning": f"An unexpected error occurred during mutation validation: {e}. Raw response: {validation_response}"})